{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9UMf5REcnQG"
      },
      "source": [
        "# Statistische Analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQIfhRWWcnQI"
      },
      "source": [
        "Auch wenn es langweilig klingt - die statistische Analyse der Daten ist extrem wichtig. Aus unserer Erfahrung würden wir vermuten, dass die meisten Analyse- und Machine Learning-Projekte scheitern, weil die Statistik der Grunddaten nicht stimmt.\n",
        "\n",
        "Diese müssen wir daher unbedingt am Anfang überprüfen. Wir werden versuchen, das so interessant wie möglich zu gestalten und dabei auch verschiedene Visualisierungen verwenden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23ph7y4hcnQQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcbERgn4cnQQ"
      },
      "outputs": [],
      "source": [
        "from IPython.core.pylabtools import figsize\n",
        "figsize(16, 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOEC8U7FcnQR"
      },
      "source": [
        "## Download der Daten\n",
        "\n",
        "Reddit ist für uns eine außerordentlich gut geeignete Plattform, weil es über ein API *erlaubt*, Daten herunterzuladen. \n",
        "\n",
        "Dazu muss man allerdings die URLs der Beiträge kennen. Leider kann man seit geraumer Zeit nicht mehr beliebig weit \"rückwärts\" blättern, so dass hier ein anderer Ansatz notwendig ist.\n",
        "\n",
        "Glücklicherweise gibt es den Dienst [pushshift.io](https://pushshift.io), der die Auflistung historischer URLs ermöglicht. Ein Aufruf von https://api.pushshift.io/reddit/submission/search/?before=1624312800&sort=desc&subreddit=technology&size=100&fields=title,created_utc,url,full_link ergibt z.B.:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W8xp81wcnQR"
      },
      "source": [
        "```javascript\n",
        "{\n",
        "    \"data\": [\n",
        "        {\n",
        "            \"created_utc\": 1624312535,\n",
        "            \"full_link\": \"https://www.reddit.com/r/technology/comments/o56nuu/back_to_the_future_imagine_your_car_paying_your/\",\n",
        "            \"title\": \"Back to the future: Imagine your car paying your parking fine for you\",\n",
        "            \"url\": \"https://www.axlenews.com/post/back-to-the-future-imagine-your-car-paying-your-parking-fine-for-you\"\n",
        "        },\n",
        "        {\n",
        "            \"created_utc\": 1624311972,\n",
        "            \"full_link\": \"https://www.reddit.com/r/technology/comments/o56glj/amazon_prime_day_is_a_nightmare_for_amazon_workers/\",\n",
        "            \"title\": \"Amazon Prime Day Is a Nightmare for Amazon Workers\",\n",
        "            \"url\": \"https://jacobinmag.com/2021/06/amazon-prime-day-workplace-safety-warehouse-osha-injuries-comp-amcare\"\n",
        "        },\n",
        "        {\n",
        "            \"created_utc\": 1624311504,\n",
        "            \"full_link\": \"https://www.reddit.com/r/technology/comments/o56a9v/how_i_found_a_vulnerability_to_hack_icloud/\",\n",
        "            \"title\": \"How I Found A Vulnerability To Hack iCloud Accounts and How Apple Reacted To It\",\n",
        "            \"url\": \"https://thezerohack.com/apple-vulnerability-bug-bounty\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcNZy5_GcnQR"
      },
      "source": [
        "Ersetzt man in `full_link` alles hinter der `id` durch `.json` (etwa https://www.reddit.com/r/technology/comments/o56a9v.json), kann man auf die strukturierten Daten von Reddit zugreifen "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sjwn9b1cnQS"
      },
      "source": [
        "```javascript\n",
        "[\n",
        "  {\n",
        "    \"kind\": \"Listing\",\n",
        "    \"data\": {\n",
        "      \"after\": null,\n",
        "      \"dist\": 1,\n",
        "      \"modhash\": \"\",\n",
        "      \"geo_filter\": \"\",\n",
        "      \"children\": [\n",
        "        {\n",
        "          \"kind\": \"t3\",\n",
        "          \"data\": {\n",
        "            \"approved_at_utc\": null,\n",
        "            \"subreddit\": \"technology\",\n",
        "            \"selftext\": \"\",\n",
        "            \"user_reports\": [],\n",
        "            \"saved\": false,\n",
        "            \"mod_reason_title\": null,\n",
        "            \"gilded\": 0,\n",
        "            \"clicked\": false,\n",
        "            \"title\": \"How I Found A Vulnerability To Hack iCloud Accounts and How Apple Reacted To It\",\n",
        "            \"link_flair_richtext\": [],\n",
        "            \"subreddit_name_prefixed\": \"r/technology\",\n",
        "            \"hidden\": false,\n",
        "            \"pwls\": 6,\n",
        "            \"link_flair_css_class\": \"general\",\n",
        "            \"downs\": 0,\n",
        "            \"thumbnail_height\": 70,\n",
        "            \"top_awarded_type\": null,\n",
        "            \"parent_whitelist_status\": \"all_ads\",\n",
        "            \"hide_score\": false,\n",
        "            \"name\": \"t3_o56a9v\",\n",
        "            \"quarantine\": false,\n",
        "            \"link_flair_text_color\": \"dark\",\n",
        "            \"upvote_ratio\": 0.89,\n",
        "            \"author_flair_background_color\": null,\n",
        "            \"subreddit_type\": \"public\",\n",
        "            \"ups\": 237,\n",
        "            \"total_awards_received\": 4,\n",
        "            \"media_embed\": {},\n",
        "            \"thumbnail_width\": 140,\n",
        "            \"author_flair_template_id\": null,\n",
        "            \"is_original_content\": false,\n",
        "            \"author_fullname\": \"t2_bw2lxmyp\",\n",
        "            \"secure_media\": null,\n",
        "            \"is_reddit_media_domain\": false,\n",
        "            \"is_meta\": false,\n",
        "[...]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua2w2ojMcnQS"
      },
      "source": [
        "Die Übernahme der Daten in eine Datenbank ist jetzt nur noch Fleißarbeit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrY0K5qgcnQS"
      },
      "source": [
        "## Einschränkung der Datenmenge des Technology-Subreddits durch *Flairs*\n",
        "\n",
        "Zuerst müssen wir einen kleinen \"Ausflug\" machen, um die Datenmenge auf die für uns interessanten Posts einzuschränken. Das Technology-Subreddit ist nämlich *riesig*. Viele Teile davon sind für die Business-Fragestellung des Fahrzeugherstellers völlig irrelevant. Dies ist ein Muster, dem man häufig in Data Science-Projekten begegnet. Entweder man hat zu wenig Daten oder zu viele.\n",
        "\n",
        "Reddit kommt uns hierbei sehr entgegen, weil es sog. *Flairs* unterstützt. Dabei kann einem Post ein Thema zugeordnet werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SetzeHjcnQT"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HiT5ShmcnQT"
      },
      "source": [
        "Das hier sichtbare Flair *Transportation* ist genau das, welches wir suchen. Leider ist die Welt allerdings nicht ganz so einfach, es gibt mehrere Schwierigkeiten:\n",
        "\n",
        "1. Das Flair *Transportation* hieß früher nur *Transport*.\n",
        "2. Die Flairs wurden von Reddit erst im Mai 2015 eingeführt.\n",
        "3. Die Flairs werden nicht von allen Autoren konsistent verwendet.\n",
        "\n",
        "All diese Probleme sind sehr typisch für Data Science-Projekte, weil häufig zunächst eine Bereinigung der Daten durchgeführt werden muss. Die Strategien unterscheiden sich dabei:\n",
        "* Punkt 1 können wir sehr einfach lösen, indem wir einfach beide Flairs berücksichtigen.\n",
        "* Punkt 2 und 3 sind deutlich schwieriger. Dazu trainieren wir einen Klassifikator und klassifizieren mit diesem einfach alle (Toplevel-) Posts neu. Die so gefundenen Posts verwenden wir dann als Datenmenge für die weitere Analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhHfmu9vcnQT"
      },
      "source": [
        "## Achtung\n",
        "\n",
        "Dieser Teil des Notebooks benötigt das gesamte Technology-Subreddit, das für den Download leider zu groß ist (10 GB). Daher ist dies im Colab-Notebook nicht ablauffähig! Eine Erklärung, wie man diese Daten selbst akquirieren kann, findet sich z.B. im Artikel [Beziehungssache](https://www.heise.de/select/ix/2021/7/2102513144636338770) in der aktuellen iX. \n",
        "\n",
        "Wir benötigen dies aber nur für die Erzeugung einer kleineren Datenmenge, die dann anschließend im Notebook ausschließlich verwendet wird und selbstverständlich heruntergeladen werden kann.\n",
        "\n",
        "Bitte ab hier deshalb vorerst nur zuschauen!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHdiTJk2cnQT"
      },
      "outputs": [],
      "source": [
        "sql = sqlite3.connect(\"technology.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqoeiWiscnQU"
      },
      "source": [
        "Wir bestimmen zunächst die Titel der Posts, die zu den Flairs *Transport* und *Transportation* gehören und nach dem 1.5.2015 gepostet wurden. Dies ist der positive Teil unserer Trainingsmenge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OcJozuecnQU"
      },
      "outputs": [],
      "source": [
        "pos = pd.read_sql(\"SELECT created_utc, title FROM posts p\\\n",
        "                            WHERE parent_id IS NULL AND \\\n",
        "                                  (flair='Transport' OR \\\n",
        "                                   flair='Transportation') AND \\\n",
        "                                  created_utc>='2015-05-01'\", \n",
        "                  sql, parse_dates=[\"created_utc\"])\n",
        "pos[\"target\"] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5UEk9ZqcnQU"
      },
      "source": [
        "Nun selektieren wir alle Posts, die zu anderen Flairs gehören. Dabei berücksichtigen wir nicht die Posts ohne Flair, weil der Autor möglicherweise das Flair einfach vergessen hat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3sq015lcnQU"
      },
      "outputs": [],
      "source": [
        "neg = pd.read_sql(\"SELECT created_utc, title FROM posts p\\\n",
        "                            WHERE parent_id IS NULL AND \\\n",
        "                                  (flair!='Transport' AND \\\n",
        "                                   flair!='Transportation' AND \\\n",
        "                                   flair IS NOT NULL) AND \\\n",
        "                                  created_utc>='2015-05-01'\", \n",
        "                  sql, parse_dates=[\"created_utc\"])\n",
        "neg[\"target\"] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFToRTzIcnQV"
      },
      "source": [
        "Wir erzeugen ein stratifizertes Datenset mit gleich vielen positiven wie negativen Beispielen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7nLwLh5cnQV"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([pos, neg.sample(n = len(pos), random_state=42)], \n",
        "                 ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWjpAdCAcnQV"
      },
      "source": [
        "Nun werden die Daten vektorisiert. Wir verzichten hier auf Stopwords etc., die üblicherweise in der Textanalyse verwendet werden und vertrauen auf die Funktionsweise von TF/IDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQHpPH35cnQV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.7, min_df=5)\n",
        "tfidf_vectors = tfidf.fit_transform(data[\"title\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQULG1g1cnQW"
      },
      "source": [
        "Als Konvention nennen wir die unabhängige Variable `X` und die abhängige `Y`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9tftNvycnQW"
      },
      "outputs": [],
      "source": [
        "X = tfidf_vectors\n",
        "Y = data[\"target\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO8hsGABcnQW"
      },
      "source": [
        "### Hold-out-Verfahren: Getrennte Mengen für Training und Test\n",
        "\n",
        "Wir teilen die Datenmenge in einen Teil, mit dem wir den Klassifikator trainieren (75%) und eine, mit dem wir die Ergebnisse verproben:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-18T20:08:34.239506Z",
          "start_time": "2018-09-18T20:08:34.227489Z"
        },
        "id": "BYMJM_3rcnQW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS8w4ANecnQW"
      },
      "source": [
        "Das Klassifikationsmodell wird als Support Vector Machine nur mit Trainingsdaten trainiert:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMJRexYPcnQX"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
        "clf.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uopvZ2-GcnQX"
      },
      "source": [
        "Wir führen eine Vorhersage für die (dem Klassifikator unbekannten) Testdaten durch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m24alxrAcnQX"
      },
      "outputs": [],
      "source": [
        "Y_predicted = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smKlRydEcnQY"
      },
      "source": [
        "Und betrachten die Ergebnisse der Klassifikation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXbsUQIvcnQY"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(Y_test, Y_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z9vnU0GcnQY"
      },
      "source": [
        "90% bzw. 96% Precision und Recall sind ziemlich gut, mit diesem Modell können wir arbeiten und nun alle Posts (größtenteils richtig) klassifizieren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M47wPx0cnQY"
      },
      "outputs": [],
      "source": [
        "all_posts = pd.read_sql(\"SELECT id, created_utc, title FROM posts p\\\n",
        "                            WHERE parent_id IS NULL\", sql, parse_dates=[\"created_utc\"])\n",
        "all_posts[\"transport\"] = clf.predict(tfidf.transform(all_posts[\"title\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5lHQt3vcnQY"
      },
      "outputs": [],
      "source": [
        "transport = all_posts[all_posts[\"transport\"] == 1]\n",
        "transport"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbs2VMH6cnQY"
      },
      "source": [
        "Nach all der Vorarbeit haben wir nun die Daten selektiert, mit denen wir jetzt ausschließlich arbeiten werden. \n",
        "\n",
        "Der Vorbereitungsaufwand mag hoch erscheinen, allerdings haben wir nun auch wirklich ein Datenset, was genau zu unseren Geschäftsanforderungen passt. Mit manuellen Methoden wäre das nicht mit vertretbarem Aufwand möglich gewesen! \n",
        "\n",
        "Das ist auch ein Grund für die Beliebtheit von Data Science: mit relativ moderatem Aufwand können genau passende Datenmenge erzeugt werden!\n",
        "\n",
        "Die Transport-Daten haben wir alle in einer SQLite-Datenbank abgespeichert, mit der wir jetzt weiterarbeiten werden. Danke für die Geduld!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTDgoERDcnQZ"
      },
      "source": [
        "## Ab hier geht es nun auch wieder mit Colab weiter!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H5Z8DePcnQZ"
      },
      "source": [
        "Die Daten des Transport-Flairs stellen wir als SQLIte-Datenbank zur Verfügung, dabei handelt es sich immer noch um eine beträchtliche Menge.\n",
        "\n",
        "Zunächst bauen wir die Verbindung zur Datenbank auf:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!test -f technology-transport-short.db || (wget https://datanizing.com/data-science-day/technology-transport-short.7z && 7z x technology-transport-short.7z && rm technology-transport-short.7z)"
      ],
      "metadata": {
        "id": "8JYrdMgMc1H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY9q-b54cnQZ"
      },
      "outputs": [],
      "source": [
        "sql = sqlite3.connect(\"technology-transport-short.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eam9mm8McnQZ"
      },
      "source": [
        "Die Datenbank selbst ist sehr einfach aufgebaut und besteht nur aus einer einzigen Tabelle (eigentlich könnte man auch mit CSV-Dateien arbeiten, aber dann müssen immer alle Daten im Speicher gehalten werden - das ist oft ungeschickt):\n",
        "\n",
        "|Feld|Typ|Attribute|\n",
        "|---|---|---|\n",
        "|id|text|not null primary key|\n",
        "|kind|text||\n",
        "|title|text||\n",
        "|link_id|text||\n",
        "|parent_id|text||\n",
        "|ups|integer||\n",
        "|downs|integer||\n",
        "|score|integer||\n",
        "|author|text||\n",
        "|num_comments|integer||\n",
        "|created_utc|timestamp||\n",
        "|permalink|text||\n",
        "|url|text||\n",
        "|text|text||\n",
        "|level|integer||\n",
        "|top_parent|text||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_z09kNCcnQZ"
      },
      "source": [
        "Bei User Generated Content oder auch anderen Textdaten ist es häufig sinnvoll, die Analyse in zwei unterschiedlichen Domänen durchzuführen.\n",
        "\n",
        "Wir betrachten zunächst die strukturierten Daten (die *Metadaten*), um zu überprüfen, ob die statistisch signifikant und valide sind.\n",
        "\n",
        "Anschließend konzentrieren wir uns auf die unstrukturierten Daten, also die Texte selbst. Dort versuchen wir herauszufinden, ob die für uns relevanten Themen abgedeckt werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrTD60QccnQZ"
      },
      "source": [
        "## Gesamtstatistik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syQ7zcdAcnQZ"
      },
      "source": [
        "Wir starten mit der Berechnung einiger Größen, die wir später immer wieder benötigen werden. Diese sind noch nicht zeitabhängig, sondern sollen uns nur einen Eindruck von der Größe der Datenmenge verschaffen.\n",
        "\n",
        "Als erstes interessiert uns die Gesamtanzahl der Posts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TnrvM4ocnQZ"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"SELECT COUNT(*) FROM posts\", sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84Wd1eHLcnQa"
      },
      "source": [
        "Das ist eine sehr große Anzahl von Posts, darin enthalten sind allerdings sowohl Initial-Posts als auch Kommentare auf Posts. \n",
        "\n",
        "Wenn wir nur die Toplevel- oder Initial-Posts betrachten wollen, so ist das leicht möglich, weil bei diesen keine `parent_id` gesetzt ist:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHRO-fN0cnQa"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"SELECT * FROM posts WHERE parent_id IS NULL\", sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHMAqGhucnQa"
      },
      "source": [
        "Auch diese Anzahl ist absolut groß genug, um daraus statistisch signifikante Aussagen ableiten zu können.\n",
        "\n",
        "Oftmals haben Foren oder UGC-Sites das Problem, dass die Inhalte zwar in großer Menge zur Verfügung stehen, aber nur von wenigen Autoren geschaffen werden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COuGD4DPcnQa"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"SELECT COUNT(DISTINCT author) FROM posts\", sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anfAzvvIcnQa"
      },
      "source": [
        "Auch hier herrscht eine große Vielfalt, was uns in der Analyse sehr hilft. Betrachten wir außerdem noch die Anazhl der Autoren, die die Toplevel-Posts erstellt haben:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvLf3qNfcnQa"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"SELECT COUNT(DISTINCT author) FROM posts WHERE parent_id IS NULL\", sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fkZwG7ycnQa"
      },
      "source": [
        "Im Verhältnis zu der Anzahl der Toplevel-Posts sind das ziemlich viele Autoren, so dass sich auch hier ein breites Meinungsbild ergibt.\n",
        "\n",
        "Schauen wir uns zuletzt noch die Namen der Autoren an, die am meisten geschrieben haben:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "CAUVp_oBcnQb"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"SELECT author, COUNT(author) AS count FROM posts \\\n",
        "             WHERE parent_id IS NULL\\\n",
        "             GROUP BY author ORDER BY count DESC LIMIT 20\", sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZbs5I4-cnQb"
      },
      "source": [
        "Wir können hier erkennen, dass es schon sehr aktive Autoren gibt. Das ist aber ein übliches Verhältnis bei sozialen Netzwerken, das Phänomen nennt sich [Ein-Prozent-Regel](https://de.wikipedia.org/wiki/Ein-Prozent-Regel_(Internet)).\n",
        "\n",
        "Bis hierher sieht also alles gut aus, die Grundwerte der Datenmenge passen!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vF8vQEcnQb"
      },
      "source": [
        "## Zeitentwicklung von Posts und Kommentaren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qZ3S44ycnQb"
      },
      "source": [
        "Nach der Business-Fragestellung möchten wir versuchen, Trends abzuleiten und vergangene Trends zu verstehen. Das geht allerdings nur, wenn die Daten hinlänglich aktuell sind. Dewegen analysieren wir zunächst den zeitlichen Verlauf der Posts.\n",
        "\n",
        "Dazu verdichten wir die Daten gleich nach Monaten, eine genauere Analyse ist für den langen Zeitraum nicht sinnvoll:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2H2-Wgi-cnQb"
      },
      "outputs": [],
      "source": [
        "time = pd.read_sql(\"SELECT STRFTIME('%Y-%m-01', created_utc) AS time, COUNT(*) AS count \\\n",
        "                    FROM posts GROUP BY time\", \n",
        "                   sql, parse_dates=[\"time\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc5ilW2BcnQb"
      },
      "outputs": [],
      "source": [
        "time.set_index(\"time\").plot(title=\"Gesamtposts pro Monat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX4CY8pucnQc"
      },
      "source": [
        "Die Grafik wirkt etwas *unruhig*, weil sich von Monat zu Monat doch größere Änderungen ergeben. Das können wir glätten, indem wir die Posts auf Quartale verdichten. `pandas` bietet uns dazu leistungsfähige Funktionen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g9MgXYZcnQc"
      },
      "outputs": [],
      "source": [
        "time.set_index(\"time\").resample('Q').sum().plot(title=\"Gesamtposts pro Quartal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLEMmTXzcnQc"
      },
      "source": [
        "Das sieht deutlich übersichtlicher aus. Es ist außerdem ein positiver Trend zu beobachten, was uns zuversichtlich stimmen sollte, dass wir statistisch valide Daten analysieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL_vXcWXcnQc"
      },
      "source": [
        "## Statistik über Autoren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZsKGfiVcnQc"
      },
      "source": [
        "Neben den Posts selbst spielen die Autoren eine große Rolle. Wie haben die sich über die Zeit entwickelt? So wäre es etwa ungünstig, wenn es immer weniger Autoren gibt. Auch hier kann uns die Datenbank viel Rechenarbeit abnehmen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osZ3I2sqcnQc"
      },
      "outputs": [],
      "source": [
        "time_author = pd.read_sql(\"SELECT STRFTIME('%Y-%m-01', created_utc) AS time, \\\n",
        "                                  COUNT(DISTINCT author) AS count \\\n",
        "                           FROM posts WHERE parent_id IS NULL GROUP BY time\", \n",
        "                          sql, parse_dates=[\"time\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNDtT5SBcnQc"
      },
      "source": [
        "Der Übersichtlichkeit halber aggregieren wir das gleich wieder für Quartale:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYUzbNs0cnQd"
      },
      "outputs": [],
      "source": [
        "time_author.set_index(\"time\").resample('Q').sum().plot(title=\"Autoren pro Quartal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G40LJFJrcnQd"
      },
      "source": [
        "Das ist sehr interessant! Offenbar hat die Anzahl der Autoren mit der Zeit abgenommen seit dem Maximum im Jahr 2012. Da die Anzahl der Posts gewachsen ist, bedeutet das, dass die durchschnittliche Anzahl von Posts pro Autor auch zugenommen haben muss. Das ist durchaus typisch für eine Experten-Community.\n",
        "\n",
        "Betrachten wir das Posting-Verhalten der Autoren noch etwas genauer und berechnen dazu eine Tabelle, in der für jeden Autor die Anzahl der Posts steht:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBKV6EN2cnQd"
      },
      "outputs": [],
      "source": [
        "cpa = pd.read_sql(\"SELECT author, COUNT(*) AS c FROM posts GROUP BY author\", sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orE853OYcnQd"
      },
      "source": [
        "Die Posts der gelöschten Accounts interessieren uns nicht (weil sich dahinter vermutlich Einzelposts vieler unterschiedlicher Autoren verbergen). Auch den `AutoModerator` lassen wir für die Analyse weg und erzeugen damit einen kleineren `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ1BF91ecnQd"
      },
      "outputs": [],
      "source": [
        "cpa = cpa[~cpa[\"author\"].isin([\"[deleted]\", \"AutoModerator\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEGeOPiqcnQd"
      },
      "source": [
        "Mithilfe der `describe`-Funktion können wir uns statistische Informationen zu der Anzahl der Posts pro Autor ausgeben lassen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOnLQU97cnQd"
      },
      "outputs": [],
      "source": [
        "cpa.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft_qNXxjcnQd"
      },
      "source": [
        "Tatsächlich hat ein einzelner Autor 3.100 Posts verfasst. Die große Mehrzahl der Autoren (mehr als die Hälfte) hat hingegen nur einmal gepostet.\n",
        "\n",
        "Das können wir uns gut in einem Histogramm darstellen lassen. Aufgrund der sehr ungleichen Verteilung wählen wir eine logarithmische Darstellung:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyFdOQohcnQe"
      },
      "outputs": [],
      "source": [
        "cpa.plot.hist(bins=80, logy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNPWCVh1cnQe"
      },
      "source": [
        "Auch wenn die Verteilung etwas merkwürdig aussieht, passt das gut! Es gibt nur vier Autoren, die mehr als 1.000 Posts geschrieben haben. Das spricht für eine gut *balancierte* Community."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgKgtC5rcnQe"
      },
      "source": [
        "## Korrelationsanalyse\n",
        "\n",
        "In Reddit können sog. *Scores* vergeben werden. Dahinter verbirgt sich die Differenz von Up- und Downvotes (die einzeln nicht mehr sichtbar sind). Wir können die Hypothese aufstellen, dass Toplevel-Posts mit einem hohen Score auch viele Kommentare auf sich ziehen.\n",
        "\n",
        "Dazu lassen wir wieder die Datenbank den Großteil der Berechnung erledigen. Um statistisch signifikante Informationen zu produzieren, betrachten wir nur Posts mit mehr als zehn Kommentaren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zao_Hdh4cnQe"
      },
      "outputs": [],
      "source": [
        "sc_top = pd.read_sql(\"SELECT p.id, AVG(p.score) AS score, COUNT(*) AS count\\\n",
        "                       FROM posts p, posts c\\\n",
        "                       WHERE p.parent_id IS NULL AnD c.parent_id=p.id \\\n",
        "                       GROUP BY p.id\\\n",
        "                       HAVING count>10\", sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFHvj_tfcnQe"
      },
      "source": [
        "Diese können wir nun in einem sog. Scatterplot darstellen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08zxm-6zcnQe"
      },
      "outputs": [],
      "source": [
        "sc_top.plot.scatter(x=\"score\", y=\"count\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGysbY0rcnQe"
      },
      "source": [
        "Hier scheint es tatsächlich eine Korrelation zu geben. Eine bessere Darstellung können wir mithilfe von `seaborn` erreichen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "QYd27tC7cnQe"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16, 16))\n",
        "sns.jointplot(x=\"score\", y=\"count\", data=sc_top, kind=\"reg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-5kGVK9cnQe"
      },
      "source": [
        "das sieht nach einem deutlichen Zusammenhang aus. Wenn wir das quantifizieren wollen, führen wir eine Regressionsanalyse durch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zivtwuR2cnQf"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "r = scipy.stats.linregress(sc_top[\"score\"], sc_top[\"count\"])\n",
        "r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3TxRKZZcnQf"
      },
      "source": [
        "Der *Pearson R*-Wert ist 1, wenn Werte total korreliert sind, -1 bei einer Antikorrelation und 0 bei unkorrelierten Werten. Auch hier kann man die Korrelation gut erkennen. `p` ist das sog. Signifikanzniveau und hier sehr klein, was für die Güte der Analyse spricht.\n",
        "\n",
        "Nachdem der Score in etwa den Likes in anderen sozialen Netzwerken entspricht, haben wir hier den bekannten Zusammenhang zwischen Likes und Comments nachgewiesen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQAEA9x2cnQf"
      },
      "source": [
        "## Inhaltliche Analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-TC2zkDcnQf"
      },
      "source": [
        "Wir kennen nun die Statistik der Metadaten und einige Korrelationen, die uns zuversichtlich stimmen, dass der Transport-Flair des Technology-Subreddit gut geeignet für unsere Analyse ist.\n",
        "\n",
        "Allerdings müssen wir noch die inhaltliche Seite überprüfen. So wäre es z.B. möglich, dass das Reddit voller Spam-Nachrichten ist oder die Diskussion trotz des Namens in eine völlig andere Richtung gehen. Dazu müssen wir die Texte analysieren.\n",
        "\n",
        "Wir laden zunächst die Title und Texte aus der Datenbank ein: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogIARSoNcnQf"
      },
      "outputs": [],
      "source": [
        "posts = pd.read_sql(\"SELECT title, text, title||' '||text AS fulltext FROM posts\", sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUsbab9RcnQf"
      },
      "source": [
        "Um die einzelnen Wörter zu zählen, ist der `Counter` aus dem `collections`-Paket von Python optimal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSd2sU6VcnQf"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLHoXYMtcnQf"
      },
      "source": [
        "Wir betrachten zunächst die Titel und müssen diese nun in Wörter zerlegen. [Tokenisierung](https://de.wikipedia.org/wiki/Tokenisierung) ist ein nicht-triviales Problem, das man normalerweise mit spezieller Software wie etwas [spaCy](https://spacy.io) lösen sollte. Das sparen wir uns allerdings hier und nutzen einen einfache `regex`-Tokenizer, weil wir sonst sehr lange auf die Ergebnisse waren müssten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kud3ZGDcnQf"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "title_counter = Counter([w.lower() for t in posts[\"title\"] for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llXNKSlzcnQg"
      },
      "source": [
        "Der `title_counter` verfügt über eine `most_common`-Funktion, mit der wir uns die häufigsten Wörter ausgeben lassen könnten. Stattdessen nutzen wir Wordclouds, die uns eine intuitive Visualisierung bieten:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qaj02V3gcnQg"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "wc.generate_from_frequencies(title_counter)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wc, interpolation='bilinear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_i-NySdcnQg"
      },
      "source": [
        "Leider kann man außer sehr allgemeinen Wörtern nicht viel erkennen. Wir müssen die Ergebnisse filtern und die sog. *Stoppworte* eliminieren. Zum Glück gibt es dazu fertige Listen, die wir hier noch etwas ergänzen: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DvAm0aLcnQg"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "for w in \"removed deleted post message account moderators http https www youtube com \\\n",
        "          watch gt look looks feel test know think go going submission link apologize \\\n",
        "          inconvenience don want automatically based buy compose good image karma like \\\n",
        "          lot need people self shit sound sounds spam submitting subreddit things \\\n",
        "          video way years time days doesn en fuck money org read reddit review \\\n",
        "          right said says subreddit subreddits sure thank try use videos wiki \\\n",
        "          wikipedia work ll thing point ve actually wait hello new amp better \\\n",
        "          isn yeah probably pretty yes didn pay long posts commenting portion \\\n",
        "          contribute questions unfortunately allowed submissions gifs pics sidebar\".split(\" \"):\n",
        "    stopwords.add(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R78j4O44cnQg"
      },
      "source": [
        "Wir nutzen diese Liste und lassen einbuchstabige Wörter auch gleich weg:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOFqsmR5cnQg"
      },
      "outputs": [],
      "source": [
        "title_counter = Counter([w for t in posts[\"title\"].str.lower()\n",
        "                            for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)\n",
        "                               if (w not in stopwords) and (len(w) > 1)\n",
        "                        ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzjLdsXTcnQg"
      },
      "source": [
        "Die Wordcloud kann wieder genauso erzeugt werden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yryt_TLOcnQg"
      },
      "outputs": [],
      "source": [
        "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "wc.generate_from_frequencies(title_counter)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wc, interpolation='bilinear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AThR9-ucnQg"
      },
      "source": [
        "Das sieht schon sehr gut aus und passt genau zu unserem Thema. Wunderbar, das bedeutet, dass wir die richtige Datenmenge ausgewählt haben und auch unsere Klassifikation gut funktioniert hat.\n",
        "\n",
        "Analysieren wir zum Vergleich noch die vollständigen Text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL5b7PF2cnQh"
      },
      "outputs": [],
      "source": [
        "text_counter = Counter([w.lower() for t in posts[\"fulltext\"].str.lower() \n",
        "                            for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)\n",
        "                               if (w not in stopwords) and (len(w) > 1)\n",
        "                        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWQaZwt_cnQh"
      },
      "outputs": [],
      "source": [
        "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "wc.generate_from_frequencies(text_counter)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wc, interpolation='bilinear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRFu9vjxcnQh"
      },
      "source": [
        "Auch das passt prima!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XgzJPBrcnQh"
      },
      "source": [
        "## Topic Modelle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M7799amcnQh"
      },
      "source": [
        "Bisher haben wir die inhaltlichen Aspekte der Posts nur durch Zählen der Wörter berücksichtigt. Allerdings interessieren uns auch Nischen-Themen, die mir mithilfe sog. [Topic Modelle](https://en.wikipedia.org/wiki/Topic_model) aufdecken können.\n",
        "\n",
        "Hierbei handelt es sich um ein unüberwachtes Machine Learning-Verfahren zur Aufdeckung der latenten Struktur großer Datenmengen.\n",
        "\n",
        "Am häufigsten wird für Topic Models die sog. [LDA-Methode](https://de.wikipedia.org/wiki/Latent_Dirichlet_Allocation) eingesetzt, die mit stochastischem Sampling funktioniert. Da die Berechnung sehr lange benötigt und es sich in vielen Projekten gezeigt hat, dass die Ergebnisse des NMF-Algorithmus oft (mindestens) genauso gut sind, nutzen wir diesen.\n",
        "\n",
        "Dafür werden die Texte im ersten Schritt mit TD/IDF vektorisiert:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54xS34n9cnQh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
        "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(posts['fulltext'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew4iMItUcnQh"
      },
      "source": [
        "Nun können wir das Topic Model instanziieren und berechnen lassen. Bei (fast) allen Topic Models müssen wir die Anzahl der Topics vorgeben. Es gibt bestimmte Metriken wie Perplexität oder Kohärenz, mit denen sich die Güte des Modells bestimmen lässt. In unserem Fall arbeiten wir einfach mit 10 Topics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8-2nVAecnQh"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF\n",
        "nmf_text_model = NMF(n_components=10, random_state=42)\n",
        "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wonaCr4cnQh"
      },
      "source": [
        "Die Berechnung dauert normalerweise keine Minute, jetzt können wir die Daten visualisieren. Dafür nutzen wir eine kleine Hilfsfunktion, die über die Topics iteriert und Wordclouds als Ergebnisse darstellt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGvYGEtKcnQi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def wordcloud_topic_model_summary(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        freq = {}\n",
        "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
        "            freq[feature_names[i].replace(\" \", \"_\")] = topic[i]\n",
        "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "        wc.generate_from_frequencies(freq)\n",
        "        plt.figure(figsize=(12,12))\n",
        "        plt.imshow(wc, interpolation='bilinear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-jTETlTcnQi"
      },
      "source": [
        "Wir können uns nun die Wordclouds für die 10 Topics aus dem Topic Model ausgeben lassen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "a2FDOMjxcnQi"
      },
      "outputs": [],
      "source": [
        "wordcloud_topic_model_summary(nmf_text_model, tfidf_text_vectorizer.get_feature_names(), 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn5sUeg0cnQi"
      },
      "source": [
        "Plötzlich können wir auch Nischenthemen erkennen, die uns vorher verborgen waren. Das ist sehr praktisch, um Ideen für Trends zu identifizieren. Hiermit können wir außerdem erkennen, ob bestimmte Wörter möglicherweise noch eliminiert werden müssen (wie z.B. `deleted post`, das sich deswegen auch in den Stopwords findet).\n",
        "\n",
        "Durch die Geschwindigkeit, mit der ein NMF-Topic Model berechnet werden kann, bietet sich diese Methode auch zur Qualitätssicherung an."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "02-Statistische Analyse.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}