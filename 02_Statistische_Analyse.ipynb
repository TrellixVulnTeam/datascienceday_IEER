{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "02-Statistische Analyse.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjJIIwGv9M0-"
      },
      "source": [
        "# Statistische Analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgexAE9C9M1K"
      },
      "source": [
        "Auch wenn es langweilig klingt - die statistische Analyse der Daten ist extrem wichtig. Aus unserer Erfahrung würden wir vermuten, dass die meisten Analyse- und Machine Learning-Projekte scheitern, weil die Statistik der Grunddaten nicht stimmt.\n",
        "\n",
        "Diese müssen wir daher unbedingt am Anfang überprüfen. Wir werden versuchen, das so interessant wie möglich zu gestalten und dabei auch verschiedene Visualisierungen verwenden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqqPYiry9M1Q"
      },
      "source": [
        "import pandas as pd\n",
        "import sqlite3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luZudai69M1U"
      },
      "source": [
        "from IPython.core.pylabtools import figsize\n",
        "figsize(16, 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGN-Nv4s9M2K"
      },
      "source": [
        "## Ab hier geht es nun auch wieder mit Colab weiter!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwdF0MEo9M2K"
      },
      "source": [
        "Die Daten des Transport-Flairs stellen wir als SQLIte-Datenbank zur Verfügung, dabei handelt es sich immer noch um eine beträchtliche Menge.\n",
        "\n",
        "Zunächst bauen wir die Verbindung zur Datenbank auf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPR4_gU79VmC"
      },
      "source": [
        "!test -f technology-transport-short.db || wget https://datanizing.com/data-science-day/technology-transport-short.7z && 7z x technology-transport-short.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpO3Z1Bb9M2M"
      },
      "source": [
        "sql = sqlite3.connect(\"technology-transport-short.db\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1dn_8Tr9M2N"
      },
      "source": [
        "Die Datenbank selbst ist sehr einfach aufgebaut und besteht nur aus einer einzigen Tabelle (eigentlich könnte man auch mit CSV-Dateien arbeiten, aber dann müssen immer alle Daten im Speicher gehalten werden - das ist oft ungeschickt):\n",
        "\n",
        "|Feld|Typ|Attribute|\n",
        "|---|---|---|\n",
        "|id|text|not null primary key|\n",
        "|kind|text||\n",
        "|title|text||\n",
        "|link_id|text||\n",
        "|parent_id|text||\n",
        "|ups|integer||\n",
        "|downs|integer||\n",
        "|score|integer||\n",
        "|author|text||\n",
        "|num_comments|integer||\n",
        "|created_utc|timestamp||\n",
        "|permalink|text||\n",
        "|url|text||\n",
        "|text|text||\n",
        "|level|integer||\n",
        "|top_parent|text||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HYyZiFG9M2O"
      },
      "source": [
        "Bei User Generated Content oder auch anderen Textdaten ist es häufig sinnvoll, die Analyse in zwei unterschiedlichen Domänen durchzuführen.\n",
        "\n",
        "Wir betrachten zunächst die strukturierten Daten (die *Metadaten*), um zu überprüfen, ob die statistisch signifikant und valide sind.\n",
        "\n",
        "Anschließend konzentrieren wir uns auf die unstrukturierten Daten, also die Texte selbst. Dort versuchen wir herauszufinden, ob die für uns relevanten Themen abgedeckt werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7CaQJn-9M2Q"
      },
      "source": [
        "## Gesamtstatistik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJc_p9Ew9M2Q"
      },
      "source": [
        "Wir starten mit der Berechnung einiger Größen, die wir später immer wieder benötigen werden. Diese sind noch nicht zeitabhängig, sondern sollen uns nur einen Eindruck von der Größe der Datenmenge verschaffen.\n",
        "\n",
        "Als erstes interessiert uns die Gesamtanzahl der Posts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_jY2EWU9M2R"
      },
      "source": [
        "pd.read_sql(\"SELECT COUNT(*) FROM posts\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbi8CCq49M2S"
      },
      "source": [
        "Das ist eine sehr große Anzahl von Posts, darin enthalten sind allerdings sowohl Initial-Posts als auch Kommentare auf Posts. \n",
        "\n",
        "Wenn wir nur die Toplevel- oder Initial-Posts betrachten wollen, so ist das leicht möglich, weil bei diesen keine `parent_id` gesetzt ist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcXoHsbt9M2T"
      },
      "source": [
        "pd.read_sql(\"SELECT COUNT(*) FROM posts WHERE parent_id IS NULL\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUogyV2O9M2U"
      },
      "source": [
        "Auch diese Anzahl ist absolut groß genug, um daraus statistisch signifikante Aussagen ableiten zu können.\n",
        "\n",
        "Oftmals haben Foren oder UGC-Sites das Problem, dass die Inhalte zwar in großer Menge zur Verfügung stehen, aber nur von wenigen Autoren geschaffen werden:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FokOd69M2W"
      },
      "source": [
        "pd.read_sql(\"SELECT COUNT(DISTINCT author) FROM posts\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp-6_VX29M2X"
      },
      "source": [
        "Auch hier herrscht eine große Vielfalt, was uns in der Analyse sehr hilft. Betrachten wir außerdem noch die Anazhl der Autoren, die die Toplevel-Posts erstellt haben:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLNcZO1E9M2Y"
      },
      "source": [
        "pd.read_sql(\"SELECT COUNT(DISTINCT author) FROM posts WHERE parent_id IS NULL\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO3JwIjE9M2Z"
      },
      "source": [
        "Im Verhältnis zu der Anzahl der Toplevel-Posts sind das ziemlich viele Autoren, so dass sich auch hier ein breites Meinungsbild ergibt.\n",
        "\n",
        "Schauen wir uns zuletzt noch die Namen der Autoren an, die am meisten geschrieben haben:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "lX-cuyIM9M2a"
      },
      "source": [
        "pd.read_sql(\"SELECT author, COUNT(author) AS count FROM posts \\\n",
        "             WHERE parent_id IS NULL\\\n",
        "             GROUP BY author ORDER BY count DESC LIMIT 20\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHy-UjFr9M2b"
      },
      "source": [
        "Wir können hier erkennen, dass es schon sehr aktive Autoren gibt. Das ist aber ein übliches Verhältnis bei sozialen Netzwerken, das Phänomen nennt sich [Ein-Prozent-Regel](https://de.wikipedia.org/wiki/Ein-Prozent-Regel_(Internet)).\n",
        "\n",
        "Bis hierher sieht also alles gut aus, die Grundwerte der Datenmenge passen!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFVoDlo89M2d"
      },
      "source": [
        "## Zeitentwicklung von Posts und Kommentaren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyU-Gmy-9M2e"
      },
      "source": [
        "Nach der Business-Fragestellung möchten wir versuchen, Trends abzuleiten und vergangene Trends zu verstehen. Das geht allerdings nur, wenn die Daten hinlänglich aktuell sind. Dewegen analysieren wir zunächst den zeitlichen Verlauf der Posts.\n",
        "\n",
        "Dazu verdichten wir die Daten gleich nach Monaten, eine genauere Analyse ist für den langen Zeitraum nicht sinnvoll:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DQxW-oP49M2z"
      },
      "source": [
        "time = pd.read_sql(\"SELECT STRFTIME('%Y-%m-01', created_utc) AS time, COUNT(*) AS count \\\n",
        "                    FROM posts GROUP BY time\", \n",
        "                   sql, parse_dates=[\"time\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz8tVjDy9M20"
      },
      "source": [
        "time.set_index(\"time\").plot(title=\"Gesamtposts pro Monat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw36roe89M21"
      },
      "source": [
        "Die Grafik wirkt etwas *unruhig*, weil sich von Monat zu Monat doch größere Änderungen ergeben. Das können wir glätten, indem wir die Posts auf Quartale verdichten. `pandas` bietet uns dazu leistungsfähige Funktionen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saNrhSxF9M23"
      },
      "source": [
        "time.set_index(\"time\").resample('Q').sum().plot(title=\"Gesamtposts pro Quartal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft5DTCFl9M24"
      },
      "source": [
        "Das sieht deutlich übersichtlicher aus. Es ist außerdem ein positiver Trend zu beobachten, was uns zuversichtlich stimmen sollte, dass wir statistisch valide Daten analysieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGWkndy19M25"
      },
      "source": [
        "## Statistik über Autoren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmshb1EW9M25"
      },
      "source": [
        "Neben den Posts selbst spielen die Autoren eine große Rolle. Wie haben die sich über die Zeit entwickelt? So wäre es etwa ungünstig, wenn es immer weniger Autoren gibt. Auch hier kann uns die Datenbank viel Rechenarbeit abnehmen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYqj9VS39M26"
      },
      "source": [
        "time_author = pd.read_sql(\"SELECT STRFTIME('%Y-%m-01', created_utc) AS time, \\\n",
        "                                  COUNT(DISTINCT author) AS count \\\n",
        "                           FROM posts WHERE parent_id IS NULL GROUP BY time\", \n",
        "                          sql, parse_dates=[\"time\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5YK0rdc9M27"
      },
      "source": [
        "Der Übersichtlichkeit halber aggregieren wir das gleich wieder für Quartale:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBf1X01S9M27"
      },
      "source": [
        "time_author.set_index(\"time\").resample('Q').sum().plot(title=\"Autoren pro Quartal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89cLE0VX9M28"
      },
      "source": [
        "Das ist sehr interessant! Offenbar hat die Anzahl der Autoren mit der Zeit abgenommen seit dem Maximum im Jahr 2012. Da die Anzahl der Posts gewachsen ist, bedeutet das, dass die durchschnittliche Anzahl von Posts pro Autor auch zugenommen haben muss. Das ist durchaus typisch für eine Experten-Community.\n",
        "\n",
        "Betrachten wir das Posting-Verhalten der Autoren noch etwas genauer und berechnen dazu eine Tabelle, in der für jeden Autor die Anzahl der Posts steht:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBPZ5yMk9M2-"
      },
      "source": [
        "cpa = pd.read_sql(\"SELECT author, COUNT(*) AS c FROM posts GROUP BY author\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_HzxIv09M2-"
      },
      "source": [
        "Die Posts der gelöschten Accounts interessieren uns nicht (weil sich dahinter vermutlich Einzelposts vieler unterschiedlicher Autoren verbergen). Auch den `AutoModerator` lassen wir für die Analyse weg und erzeugen damit einen kleineren `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeIYEdGs9M2_"
      },
      "source": [
        "cpa = cpa[~cpa[\"author\"].isin([\"[deleted]\", \"AutoModerator\"])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuRhH4rX9M3A"
      },
      "source": [
        "Mithilfe der `describe`-Funktion können wir uns statistische Informationen zu der Anzahl der Posts pro Autor ausgeben lassen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3LwOuzN9M3B"
      },
      "source": [
        "cpa.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syYRahng9M3C"
      },
      "source": [
        "Tatsächlich hat ein einzelner Autor 3.100 Posts verfasst. Die große Mehrzahl der Autoren (mehr als die Hälfte) hat hingegen nur einmal gepostet.\n",
        "\n",
        "Das können wir uns gut in einem Histogramm darstellen lassen. Aufgrund der sehr ungleichen Verteilung wählen wir eine logarithmische Darstellung:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-O27CQr9M3D"
      },
      "source": [
        "cpa.plot.hist(bins=80, logy=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c74hMxra9M3D"
      },
      "source": [
        "Auch wenn die Verteilung etwas merkwürdig aussieht, passt das gut! Es gibt nur vier Autoren, die mehr als 1.000 Posts geschrieben haben. Das spricht für eine gut *balancierte* Community."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeYhNCRB9M3E"
      },
      "source": [
        "## Korrelationsanalyse\n",
        "\n",
        "In Reddit können sog. *Scores* vergeben werden. Dahinter verbirgt sich die Differenz von Up- und Downvotes (die einzeln nicht mehr sichtbar sind). Wir können die Hypothese aufstellen, dass Toplevel-Posts mit einem hohen Score auch viele Kommentare auf sich ziehen.\n",
        "\n",
        "Dazu lassen wir wieder die Datenbank den Großteil der Berechnung erledigen. Um statistisch signifikante Informationen zu produzieren, betrachten wir nur Posts mit mehr als zehn Kommentaren:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suqaHM3S9M3F"
      },
      "source": [
        "sc_top = pd.read_sql(\"SELECT p.id, AVG(p.score) AS score, COUNT(*) AS count\\\n",
        "                       FROM posts p, posts c\\\n",
        "                       WHERE p.parent_id IS NULL AnD c.parent_id=p.id \\\n",
        "                       GROUP BY p.id\\\n",
        "                       HAVING count>10\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jws92qEd9M3G"
      },
      "source": [
        "Diese können wir nun in einem sog. Scatterplot darstellen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyMHcXwu9M3G"
      },
      "source": [
        "sc_top.plot.scatter(x=\"score\", y=\"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7AkKsIc9M3I"
      },
      "source": [
        "Hier scheint es tatsächlich eine Korrelation zu geben. Eine bessere Darstellung können wir mithilfe von `seaborn` erreichen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0pm6NogF9M3J"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16, 16))\n",
        "sns.jointplot(x=\"score\", y=\"count\", data=sc_top, kind=\"reg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fATbxPYl9M3L"
      },
      "source": [
        "das sieht nach einem deutlichen Zusammenhang aus. Wenn wir das quantifizieren wollen, führen wir eine Regressionsanalyse durch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIZorbhZ9M3L"
      },
      "source": [
        "import scipy.stats\n",
        "r = scipy.stats.linregress(sc_top[\"score\"], sc_top[\"count\"])\n",
        "r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJLQoh_T9M3N"
      },
      "source": [
        "Der *Pearson R*-Wert ist 1, wenn Werte total korreliert sind, -1 bei einer Antikorrelation und 0 bei unkorrelierten Werten. Auch hier kann man die Korrelation gut erkennen. `p` ist das sog. Signifikanzniveau und hier sehr klein, was für die Güte der Analyse spricht.\n",
        "\n",
        "Nachdem der Score in etwa den Likes in anderen sozialen Netzwerken entspricht, haben wir hier den bekannten Zusammenhang zwischen Likes und Comments nachgewiesen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqOofOF-9M3O"
      },
      "source": [
        "## Inhaltliche Analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzdxbDlt9M3O"
      },
      "source": [
        "Wir kennen nun die Statistik der Metadaten und einige Korrelationen, die uns zuversichtlich stimmen, dass der Transport-Flair des Technology-Subreddit gut geeignet für unsere Analyse ist.\n",
        "\n",
        "Allerdings müssen wir noch die inhaltliche Seite überprüfen. So wäre es z.B. möglich, dass das Reddit voller Spam-Nachrichten ist oder die Diskussion trotz des Namens in eine völlig andere Richtung gehen. Dazu müssen wir die Texte analysieren.\n",
        "\n",
        "Wir laden zunächst die Title und Texte aus der Datenbank ein: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM7U-I569M3P"
      },
      "source": [
        "posts = pd.read_sql(\"SELECT title, text, title||' '||text AS fulltext FROM posts\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2cPoCD9M3Q"
      },
      "source": [
        "Um die einzelnen Wörter zu zählen, ist der `Counter` aus dem `collections`-Paket von Python optimal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IhJeimA9M3Q"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVG9bc3q9M3R"
      },
      "source": [
        "Wir betrachten zunächst die Titel und müssen diese nun in Wörter zerlegen. [Tokenisierung](https://de.wikipedia.org/wiki/Tokenisierung) ist ein nicht-triviales Problem, das man normalerweise mit spezieller Software wie etwas [spaCy](https://spacy.io) lösen sollte. Das sparen wir uns allerdings hier und nutzen einen einfache `regex`-Tokenizer, weil wir sonst sehr lange auf die Ergebnisse waren müssten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AqYB73D9M3R"
      },
      "source": [
        "import regex as re\n",
        "title_counter = Counter([w.lower() for t in posts[\"title\"] for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhZNouSm9M3S"
      },
      "source": [
        "Der `title_counter` verfügt über eine `most_common`-Funktion, mit der wir uns die häufigsten Wörter ausgeben lassen könnten. Stattdessen nutzen wir Wordclouds, die uns eine intuitive Visualisierung bieten:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH35YEHB9M3S"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "wc.generate_from_frequencies(title_counter)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wc, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDTRDzQz9M3T"
      },
      "source": [
        "Leider kann man außer sehr allgemeinen Wörtern nicht viel erkennen. Wir müssen die Ergebnisse filtern und die sog. *Stoppworte* eliminieren. Zum Glück gibt es dazu fertige Listen, die wir hier noch etwas ergänzen: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuR6Km4P9M3T"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "for w in \"removed deleted post message account moderators http https www youtube com \\\n",
        "          watch gt look looks feel test know think go going submission link apologize \\\n",
        "          inconvenience don want automatically based buy compose good image karma like \\\n",
        "          lot need people self shit sound sounds spam submitting subreddit things \\\n",
        "          video way years time days doesn en fuck money org read reddit review \\\n",
        "          right said says subreddit subreddits sure thank try use videos wiki \\\n",
        "          wikipedia work ll thing point ve actually wait hello new amp better \\\n",
        "          isn yeah probably pretty yes didn pay long posts commenting portion \\\n",
        "          contribute questions unfortunately allowed submissions gifs pics sidebar\".split(\" \"):\n",
        "    stopwords.add(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4N_lLDt9M3U"
      },
      "source": [
        "Wir nutzen diese Liste und lassen einbuchstabige Wörter auch gleich weg:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRrq50Wi9M3V"
      },
      "source": [
        "title_counter = Counter([w for t in posts[\"title\"].str.lower()\n",
        "                            for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)\n",
        "                               if (w not in stopwords) and (len(w) > 1)\n",
        "                        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-YGfIJk9M3W"
      },
      "source": [
        "Die Wordcloud kann wieder genauso erzeugt werden:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XadbrA2t9M3W"
      },
      "source": [
        "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "wc.generate_from_frequencies(title_counter)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wc, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZd0Jwgb9M3X"
      },
      "source": [
        "Das sieht schon sehr gut aus und passt genau zu unserem Thema. Wunderbar, das bedeutet, dass wir die richtige Datenmenge ausgewählt haben und auch unsere Klassifikation gut funktioniert hat.\n",
        "\n",
        "Analysieren wir zum Vergleich noch die vollständigen Text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbjT0-bR9M3Y"
      },
      "source": [
        "text_counter = Counter([w.lower() for t in posts[\"fulltext\"].str.lower() \n",
        "                            for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)\n",
        "                               if (w not in stopwords) and (len(w) > 1)\n",
        "                        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OmTldAi9M3Y"
      },
      "source": [
        "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "wc.generate_from_frequencies(text_counter)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wc, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM_t3fOu9M3Z"
      },
      "source": [
        "Auch das passt prima!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnAmZGgm9M3a"
      },
      "source": [
        "## Topic Modelle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unMit3AS9M3b"
      },
      "source": [
        "Bisher haben wir die inhaltlichen Aspekte der Posts nur durch Zählen der Wörter berücksichtigt. Allerdings interessieren uns auch Nischen-Themen, die mir mithilfe sog. [Topic Modelle](https://en.wikipedia.org/wiki/Topic_model) aufdecken können.\n",
        "\n",
        "Hierbei handelt es sich um ein unüberwachtes Machine Learning-Verfahren zur Aufdeckung der latenten Struktur großer Datenmengen.\n",
        "\n",
        "Am häufigsten wird für Topic Models die sog. [LDA-Methode](https://de.wikipedia.org/wiki/Latent_Dirichlet_Allocation) eingesetzt, die mit stochastischem Sampling funktioniert. Da die Berechnung sehr lange benötigt und es sich in vielen Projekten gezeigt hat, dass die Ergebnisse des NMF-Algorithmus oft (mindestens) genauso gut sind, nutzen wir diesen.\n",
        "\n",
        "Dafür werden die Texte im ersten Schritt mit TD/IDF vektorisiert:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUJvINVt9M3c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
        "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(posts['fulltext'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv1K3IoG9M3d"
      },
      "source": [
        "Nun können wir das Topic Model instanziieren und berechnen lassen. Bei (fast) allen Topic Models müssen wir die Anzahl der Topics vorgeben. Es gibt bestimmte Metriken wie Perplexität oder Kohärenz, mit denen sich die Güte des Modells bestimmen lässt. In unserem Fall arbeiten wir einfach mit 10 Topics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGZqGBvq9M3d"
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "nmf_text_model = NMF(n_components=10, random_state=42)\n",
        "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDxpRh-E9M3e"
      },
      "source": [
        "Die Berechnung dauert normalerweise keine Minute, jetzt können wir die Daten visualisieren. Dafür nutzen wir eine kleine Hilfsfunktion, die über die Topics iteriert und Wordclouds als Ergebnisse darstellt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDz8Rm8I9M3e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def wordcloud_topic_model_summary(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        freq = {}\n",
        "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
        "            freq[feature_names[i].replace(\" \", \"_\")] = topic[i]\n",
        "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "        wc.generate_from_frequencies(freq)\n",
        "        plt.figure(figsize=(12,12))\n",
        "        plt.imshow(wc, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmFh7_so9M3f"
      },
      "source": [
        "Wir können uns nun die Wordclouds für die 10 Topics aus dem Topic Model ausgeben lassen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sHzNt9zw9M3f"
      },
      "source": [
        "wordcloud_topic_model_summary(nmf_text_model, tfidf_text_vectorizer.get_feature_names(), 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJM7If7j9M3f"
      },
      "source": [
        "Plötzlich können wir auch Nischenthemen erkennen, die uns vorher verborgen waren. Das ist sehr praktisch, um Ideen für Trends zu identifizieren. Hiermit können wir außerdem erkennen, ob bestimmte Wörter möglicherweise noch eliminiert werden müssen (wie z.B. `deleted post`, das sich deswegen auch in den Stopwords findet).\n",
        "\n",
        "Durch die Geschwindigkeit, mit der ein NMF-Topic Model berechnet werden kann, bietet sich diese Methode auch zur Qualitätssicherung an."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUDhshBW_Fi2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}